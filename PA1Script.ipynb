{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Linear Regression with Direct Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 1\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 1')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnOLERegression(X,y):\n",
    "    # Inputs:                                                         \n",
    "    # X = N x d \n",
    "    # y = N x 1                                                               \n",
    "    # Output: \n",
    "    # w = d x 1 \n",
    "    #Least squares estimate: w (X^T.X)^(-1).X^T.y\n",
    "    \n",
    "    Xt_X = np.dot(np.transpose(X), X) #X^T.X\n",
    "    inv_prod = np.linalg.inv(Xt_X) #(X^T.X)^(-1)\n",
    "    Xt_y = np.dot(np.transpose(X), y) #X^T.y\n",
    "    w = np.dot(inv_prod, Xt_y) #(X^T.X)^(-1).X^T.y\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOLERegression(w,Xtest,ytest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # ytest = N x 1\n",
    "    # Output:\n",
    "    # rmse = scalar value\n",
    "    #RMSE: formula (3)\n",
    "    rmse = 0\n",
    "    inner = np.subtract(ytest, np.dot(Xtest,w)) \n",
    "    rmse = np.sqrt(np.mean(inner**2)) #(inner^2/n)^(.5)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE without intercept on train data - 138.20\n",
      "RMSE with intercept on train data - 46.77\n",
      "RMSE without intercept on test data - 326.76\n",
      "RMSE with intercept on test data - 60.89\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "x1 = np.ones((len(Xtrain),1))\n",
    "x2 = np.ones((len(Xtest),1))\n",
    "\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "w = learnOLERegression(Xtrain,ytrain)\n",
    "w_i = learnOLERegression(Xtrain_i,ytrain)\n",
    "\n",
    "rmse = testOLERegression(w,Xtrain,ytrain)\n",
    "rmse_i = testOLERegression(w_i,Xtrain_i,ytrain)\n",
    "print('RMSE without intercept on train data - %.2f'%rmse)\n",
    "print('RMSE with intercept on train data - %.2f'%rmse_i)\n",
    "\n",
    "rmse = testOLERegression(w,Xtest,ytest)\n",
    "rmse_i = testOLERegression(w_i,Xtest_i,ytest)\n",
    "print('RMSE without intercept on test data - %.2f'%rmse)\n",
    "print('RMSE with intercept on test data - %.2f'%rmse_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 2\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 2')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionObjVal(w, X, y):\n",
    "\n",
    "    # compute squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y      \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar value\n",
    "    # Error: J(w) = .5(y-Xw)^T(t-Xw)\n",
    "    error = 0\n",
    "    w = w[:,np.newaxis] #w to matrix form\n",
    "    prod_1 = np.subtract(y, np.dot(X,w))\n",
    "    error = .5*np.dot(np.transpose(prod_1), prod_1)\n",
    "    # print(error)\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionGradient(w, X, y):\n",
    "\n",
    "    # compute gradient of squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y   \n",
    "    \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # gradient = d length vector (not a d x 1 matrix)\n",
    "    #Gradient: X^TXw-X^Ty\n",
    "    error_grad = np.zeros((X.shape[1],))\n",
    "\n",
    "    w = w[:,np.newaxis] #w to matrix form\n",
    "    prod_1 = np.dot(np.transpose(X), np.dot(X,w)) #X^T . Xw\n",
    "    prod_2 = np.dot(np.transpose(X),y) #X^T.y\n",
    "    gradient = np.subtract(prod_1,prod_2) #X^Tw-X^Ty\n",
    "    error_grad = gradient.flatten() #matrix to vector\n",
    "    return error_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Linear Regression RMSE on train data - 48.10\n",
      "Gradient Descent Linear Regression RMSE on test data - 54.98\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "rmse = testOLERegression(w,Xtrain_i,ytrain)\n",
    "print('Gradient Descent Linear Regression RMSE on train data - %.2f'%rmse)\n",
    "rmse = testOLERegression(w,Xtest_i,ytest)\n",
    "print('Gradient Descent Linear Regression RMSE on test data - %.2f'%rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 - Perceptron using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 3')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLinearModel(w,Xtest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # Output:\n",
    "    # ypred = N x 1 vector of predictions\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    ypred = np.zeros([Xtest.shape[0],1])\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateLinearModel(w,Xtest,ytest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # ytest = N x 1\n",
    "    # Output:\n",
    "    # acc = scalar values\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    acc = 0\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = np.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Perceptron Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Perceptron Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 - Logistic Regression Using Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 4')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticObjVal(w, X, y):\n",
    "\n",
    "    # compute log-loss error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar\n",
    "    \n",
    "    \n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    error = 0\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticGradient(w, X, y):\n",
    "\n",
    "    # compute the gradient of the log-loss error (vector) with respect\n",
    "    # to w (vector) for the given data X and y  \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = d length gradient vector (not a d x 1 matrix)\n",
    "\n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    gradient = np.zeros((w.shape[0],))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticHessian(w, X, y):\n",
    "\n",
    "    # compute the Hessian of the log-loss error (matrix) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # Hessian = d x d matrix\n",
    "    \n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    hessian = np.eye(X.shape[1])\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = np.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(logisticObjVal, w_init, jac=logisticGradient, hess=logisticHessian, args=args,method='Newton-CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = np.reshape(w,[len(w),1])\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Logistic Regression Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Logistic Regression Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 - Support Vector Machines Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 5')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSGDSVM(X,y,T,eta=0.01):\n",
    "    # learn a linear SVM by implementing the SGD algorithm\n",
    "    #\n",
    "    # Inputs:\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # T = number of iterations\n",
    "    # eta = learning rate\n",
    "    # Output:\n",
    "    # weight vector, w = d x 1\n",
    "    \n",
    "    # IMPLEMENT THIS METHOD\n",
    "    w = np.zeros([X.shape[1],1])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = np.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "w = trainSGDSVM(Xtrain_i,ytrain,100,0.01)\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('SVM Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('SVM Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6 - Plotting decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Problem 6')\n",
    "print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBoundaries(w,X,y):\n",
    "    # plotting boundaries\n",
    "\n",
    "    mn = np.min(X,axis=0)\n",
    "    mx = np.max(X,axis=0)\n",
    "    x1 = np.linspace(mn[1],mx[1],100)\n",
    "    x2 = np.linspace(mn[2],mx[2],100)\n",
    "    xx1,xx2 = np.meshgrid(x1,x2)\n",
    "    xx = np.zeros((x1.shape[0]*x2.shape[0],2))\n",
    "    xx[:,0] = xx1.ravel()\n",
    "    xx[:,1] = xx2.ravel()\n",
    "    xx_i = np.concatenate((np.ones((xx.shape[0],1)), xx), axis=1)\n",
    "    ypred = predictLinearModel(w,xx_i)\n",
    "    ax.contourf(x1,x2,ypred.reshape((x1.shape[0],x2.shape[0])),alpha=0.3,cmap='cool')\n",
    "    ax.scatter(X[:,1],X[:,2],c=y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plotBoundaries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4b5f3fd0c5c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mplotBoundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_perceptron\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXtrain_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Perceptron'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plotBoundaries' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = np.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "# Replace next three lines with code for learning w using the three methods\n",
    "w_perceptron = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_logistic = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_svm = np.zeros((Xtrain_i.shape[1],1))\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "\n",
    "ax = plt.subplot(1,3,1)\n",
    "plotBoundaries(w_perceptron,Xtrain_i,ytrain)\n",
    "ax.set_title('Perceptron')\n",
    "\n",
    "ax = plt.subplot(1,3,2)\n",
    "plotBoundaries(w_logistic,Xtrain_i,ytrain)\n",
    "ax.set_title('Logistic Regression')\n",
    "\n",
    "ax = plt.subplot(1,3,3)\n",
    "plotBoundaries(w_svm,Xtrain_i,ytrain)\n",
    "ax.set_title('SVM')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
